import requests
import json
import time
import os
import sys
from openai import OpenAI

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

try:
    from config_manager import config
except ImportError as e:
    print(f"æ— æ³•å¯¼å…¥é…ç½®ç®¡ç†å™¨: {e}")
    print("è¯·ç¡®ä¿config_manager.pyæ–‡ä»¶å­˜åœ¨ä¸”è¯­æ³•æ­£ç¡®")
    sys.exit(1)

# --- é…ç½® ---
# é€šä¹‰APIç›´è¿é…ç½® (ä» vllm_proxy_server.py è·å–)
TONGYI_API_KEY = "sk-354859a6d3ae438fb8ab9b98194f5266"
TONGYI_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
TONGYI_MODEL = "qwen-plus" # é€šä¹‰APIä½¿ç”¨çš„æ¨¡å‹

# ngrokä»£ç†æœåŠ¡å™¨é…ç½® (ä» test_client_ngrok.py è·å–)
NGROK_PROXY_URL = "https://13a8-125-220-159-5.ngrok-free.app"
DEFAULT_MODEL = config.default_model # ä» config_manager è·å–é»˜è®¤æ¨¡å‹

TEST_PROMPT_SHORT = "è¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½ã€‚"
TEST_PROMPT_LONG = "è¯·è¯¦ç»†ä»‹ç»äººå·¥æ™ºèƒ½çš„èµ·æºã€å‘å±•ã€ä¸»è¦æŠ€æœ¯ï¼ˆå¦‚æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ï¼‰ã€åº”ç”¨é¢†åŸŸã€é¢ä¸´çš„æŒ‘æˆ˜ä»¥åŠæœªæ¥å±•æœ›ã€‚è¯·ç¡®ä¿å†…å®¹ä¸°å¯Œï¼Œå­—æ•°åœ¨500å­—ä»¥ä¸Šã€‚"
NUM_ITERATIONS = 5 # æ¯ä¸ªæµ‹è¯•åœºæ™¯çš„è¿è¡Œæ¬¡æ•°

def calculate_average(times):
    """è®¡ç®—å¹³å‡æ—¶é—´"""
    if not times:
        return 0
    return sum(times) / len(times)

def run_test_scenario(name, test_func, prompt, is_streaming=False):
    """è¿è¡Œä¸€ä¸ªæµ‹è¯•åœºæ™¯å¹¶æ”¶é›†å“åº”æ—¶é—´"""
    print(f"\n=== è¿è¡Œ {name} æµ‹è¯• ({'æµå¼' if is_streaming else 'éæµå¼'}) ===")
    times = []
    for i in range(NUM_ITERATIONS):
        print(f"  - ç¬¬ {i+1}/{NUM_ITERATIONS} æ¬¡æµ‹è¯•...")
        try:
            start_time = time.time()
            full_response_content = test_func(prompt)
            end_time = time.time()
            response_time = end_time - start_time
            times.append(response_time)
            print(f"    å“åº”æ—¶é—´: {response_time:.2f}ç§’")
            if not is_streaming and full_response_content:
                print(f"    å“åº”é•¿åº¦: {len(full_response_content)} å­—ç¬¦")
            elif is_streaming:
                print(f"    æµå¼å†…å®¹æ¥æ”¶å®Œæˆã€‚")
        except Exception as e:
            print(f"    æµ‹è¯•å¤±è´¥: {e}")
            times.append(float('inf')) # æ ‡è®°ä¸ºå¤±è´¥ï¼Œä¸è®¡å…¥å¹³å‡
    
    avg_time = calculate_average([t for t in times if t != float('inf')])
    print(f"--- {name} å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’ (æˆåŠŸ {len([t for t in times if t != float('inf')])}/{NUM_ITERATIONS} æ¬¡) ---")
    return times

# --- é€šä¹‰APIç›´è¿æµ‹è¯•å‡½æ•° ---
def test_tongyi_non_stream_chat(prompt):
    """ç›´æ¥è°ƒç”¨é€šä¹‰APIè¿›è¡Œéæµå¼èŠå¤©"""
    client = OpenAI(api_key=TONGYI_API_KEY, base_url=TONGYI_BASE_URL)
    messages = [{"role": "user", "content": prompt}]
    completion = client.chat.completions.create(
        model=TONGYI_MODEL,
        messages=messages,
        stream=False,
        temperature=0.7,
        max_tokens=2000
    )
    return completion.choices[0].message.content

def test_tongyi_stream_chat(prompt):
    """ç›´æ¥è°ƒç”¨é€šä¹‰APIè¿›è¡Œæµå¼èŠå¤©"""
    client = OpenAI(api_key=TONGYI_API_KEY, base_url=TONGYI_BASE_URL)
    messages = [{"role": "user", "content": prompt}]
    full_response_content = ""
    completion = client.chat.completions.create(
        model=TONGYI_MODEL,
        messages=messages,
        stream=True,
        temperature=0.7,
        max_tokens=2000
    )
    for chunk in completion:
        if chunk.choices[0].delta.content:
            full_response_content += chunk.choices[0].delta.content
    return full_response_content

# --- ngrokä»£ç†æœ¬åœ°vLLMæµ‹è¯•å‡½æ•° ---
def test_ngrok_non_stream_chat(prompt):
    """é€šè¿‡ngrokä»£ç†è°ƒç”¨æœ¬åœ°vLLMè¿›è¡Œéæµå¼èŠå¤©"""
    payload = {
        "messages": [{"role": "user", "content": prompt}],
        "model": DEFAULT_MODEL,
        "max_tokens": 2000,
        "temperature": 0.7,
        "stream": False
    }
    response = requests.post(
        f"{NGROK_PROXY_URL}/chat",
        headers={"Content-Type": "application/json"},
        json=payload
    )
    response.raise_for_status()
    return response.json().get("response")

def test_ngrok_stream_chat(prompt):
    """é€šè¿‡ngrokä»£ç†è°ƒç”¨æœ¬åœ°vLLMè¿›è¡Œæµå¼èŠå¤©"""
    payload = {
        "messages": [{"role": "user", "content": prompt}],
        "model": DEFAULT_MODEL,
        "max_tokens": 2000,
        "temperature": 0.7,
        "stream": True
    }
    full_response_content = ""
    response = requests.post(
        f"{NGROK_PROXY_URL}/chat",
        headers={"Content-Type": "application/json"},
        json=payload,
        stream=True
    )
    response.raise_for_status()
    for chunk in response.iter_lines():
        if chunk:
            decoded_chunk = chunk.decode('utf-8')
            if decoded_chunk == "data: [DONE]": # ç»“æŸæ ‡è®°
                break
            elif decoded_chunk.startswith("data:"):
                try:
                    json_data = json.loads(decoded_chunk[5:].strip())
                    if json_data.get("choices") and len(json_data["choices"]) > 0:
                        delta = json_data["choices"][0].get("delta")
                        if delta and delta.get("content"):
                            full_response_content += delta["content"]
                except json.JSONDecodeError:
                    print(f"Warning: Could not decode JSON from chunk: {decoded_chunk}")
    return full_response_content

def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹è¿è¡Œæ€§èƒ½æµ‹è¯•...")
    print(f"æ¯ä¸ªæµ‹è¯•åœºæ™¯è¿è¡Œ {NUM_ITERATIONS} æ¬¡")
    print(f"ngrokä»£ç†åœ°å€: {NGROK_PROXY_URL}")
    print(f"é»˜è®¤æ¨¡å‹ (ngrok): {DEFAULT_MODEL}")
    print(f"é€šä¹‰APIåœ°å€: {TONGYI_BASE_URL}")
    print(f"é€šä¹‰æ¨¡å‹: {TONGYI_MODEL}")
    print("=" * 60)

    # çŸ­æ–‡æœ¬æµ‹è¯•
    print("\n### çŸ­æ–‡æœ¬æµ‹è¯• ###")
    run_test_scenario("é€šä¹‰ç›´è¿", test_tongyi_non_stream_chat, TEST_PROMPT_SHORT, is_streaming=False)
    run_test_scenario("é€šä¹‰ç›´è¿", test_tongyi_stream_chat, TEST_PROMPT_SHORT, is_streaming=True)
    run_test_scenario("ngrokä»£ç†", test_ngrok_non_stream_chat, TEST_PROMPT_SHORT, is_streaming=False)
    run_test_scenario("ngrokä»£ç†", test_ngrok_stream_chat, TEST_PROMPT_SHORT, is_streaming=True)

    # é•¿æ–‡æœ¬æµ‹è¯•
    print("\n### é•¿æ–‡æœ¬æµ‹è¯• ###")
    run_test_scenario("é€šä¹‰ç›´è¿", test_tongyi_non_stream_chat, TEST_PROMPT_LONG, is_streaming=False)
    run_test_scenario("é€šä¹‰ç›´è¿", test_tongyi_stream_chat, TEST_PROMPT_LONG, is_streaming=True)
    run_test_scenario("ngrokä»£ç†", test_ngrok_non_stream_chat, TEST_PROMPT_LONG, is_streaming=False)
    run_test_scenario("ngrokä»£ç†", test_ngrok_stream_chat, TEST_PROMPT_LONG, is_streaming=True)

    print("\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main() 